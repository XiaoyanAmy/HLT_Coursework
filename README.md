# Human Language Techlogy Project: Sentimental Analysis
This repository contains the project for the course Human Language Technology. The primary objective of this project is to address the sentiment analysis task using pretrained Large Language Models, such as BERT, with a focus on fine-tuning the model not only for parameter efficiency but also for data efficiency.

In order to enhance model tuning efficiency, we have primarily delved into the realm of continuous prompts methods. More specifically, we have experimented with two types of prompt tuning: prompt tuning [1] and prefix prompt [2]. To further enhance performance in a low-data setting, we have integrated deep metric learning with the prompt methods.


# References
[1] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

[2] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics.
