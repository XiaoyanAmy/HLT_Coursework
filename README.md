# HLT_Coursework
This repository contains the project for the course Human Language Technology. The primary objective of this project is to address the sentiment analysis task using pretrained Large Language Models, such as BERT, with a focus on fine-tuning the model not only for parameter efficiency but also for data efficiency.

In order to enhance model tuning efficiency, we have primarily delved into the realm of continuous prompts methods. More specifically, we have experimented with two types of prompt tuning: prompt tuning[1] and prefix prompt [2]. To further enhance performance in a low-data setting, we have integrated deep metric learning with the prompt methods.


# References
[1] BrianLester, Rami Al-Rfou, andNoahConstant. Thepower ofscale for parameter-eﬃcient prompt tuning. InProceedings ofthe 2021 Conference onEmpiricalMethods inNaturalLanguage Process- ing, pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

[2] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In Proceedings ofthe 59th AnnualMeeting ofthe Associationfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics.
