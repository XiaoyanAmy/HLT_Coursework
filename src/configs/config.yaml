
seed: 42
sampling_random_state: 42
# num_samplings: the number of samplings to train/validate the model
# in a low datasetting. Empty represent all data
num_samplings_train: 200
num_samplings_val: 200
# Available model_name to choose from: bert-base-uncased, roberta-base, microsoft/deberta-base
# and also their large versions: bert-large-uncased, roberta-large, microsoft/deberta-large. If you use large versions,
# please modify the num_hidden_states to 1024, otherwise, leave it to 768. If you want to use other pretrained model 
# than the ones mentioned here,please modify num_hidden_states accordingly. 
model_name: roberta-large
# tuning method can be chosen among {0, 1, 2},
# where 0 stands for fine tuning
# 1 stands for prompt tuning, and 
# 2 stands for prefix tuning 
tuning_method: 1
model_save_name: all_roberta_li1.pth
learning_rate: 9.0e-3
classifier: li # please choose from {li, dml}, li stands for linear layer, dml stands for deep metric learning
num_virtual_tokens : 7
batch_size: 8
epoch: 100
num_hidden_states: 1024