
seed: 42
sampling_random_state: 42
# num_samplings: the number of samplings to train/validate the model
# in a low datasetting. Empty represent all data
num_samplings_train: 200
num_samplings_val: 200
# Available model_name to choose from: bert-base-uncased, roberta-base, microsoft/deberta-base
# and also their large versions: bert-large-uncased, roberta-large, microsoft/deberta-large, if you use large models
# please modify the num_hidden_states to 1024, otherwise, leave it to 768. 
model_name: bert-large-uncased
# tuning method can be chosen among {0, 1, 2},
# where 0 stands for fine tuning
# 1 stands for prompt tuning, and 
# 2 stands for prefix tuning 
tuning_method: 1
model_save_name: all_roberta_li1.pth
learning_rate: 2.0e-2
classifier: li # please choose from {li, dml}, li stands for linear layer, dml stands for deep metric learning
num_virtual_tokens : 7
batch_size: 8
epoch: 50
num_hidden_states: 1024